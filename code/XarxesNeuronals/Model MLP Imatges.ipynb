{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #should be installed by default in any colab notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = \n",
    "val_set = \n",
    "\n",
    "print(train_set.data.size(), val_set.data.size())\n",
    "print(train_set.targets.size(), val_set.targets.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCModel(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FCModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "          nn.Linear(input_size, n_hidden),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden, n_hidden),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(model):\n",
    "    for name, w in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            nn.init.ones_(w)\n",
    "\n",
    "        if \"bias\" in name:\n",
    "            nn.init.zeros_(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "lambda_l2 = 1e-5\n",
    "torch.manual_seed(0) # seed for reproductibility\n",
    "\n",
    "input_size  = 28*28   # images are 28x28 pixels\n",
    "output_size = 10      # there are 10 classes\n",
    "\n",
    "model = FCModel(input_size, 128, output_size)\n",
    "\n",
    "# utility function to count number of parameters in a model\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.numel()\n",
    "        # print(p.numel())\n",
    "        # print(np)\n",
    "    return np\n",
    "\n",
    "print(f\"Number of parameters {get_n_params(model)}:\")\n",
    "\n",
    "\n",
    "# nn package also has different loss functions.\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss() # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "\n",
    "# we use the optim package to apply\n",
    "# stochastic gradient descent for our parameter updates\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# WARNING! What are we doing here?\n",
    "initialize_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # prevent this function from computing gradients\n",
    "def validate(criterion, model, loader):\n",
    "\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for data, target in loader:\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        data = data.view(-1, 28*28)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        val_loss += loss.item()\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    val_loss /= len(loader.dataset)\n",
    "    accuracy = 100. * correct / len(loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(loader.dataset),\n",
    "        accuracy))\n",
    "\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train(epoch, criterion, model, optimizer, loader):\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        data = data.view(-1, 28*28)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print loss every N iterations\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(loader.dataset),\n",
    "                100. * batch_idx / len(loader), loss.item()))\n",
    "\n",
    "\n",
    "        total_loss += loss.item()  #.item() is very important here? Why?\n",
    "\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\"train\": [], \"val\": []}\n",
    "for epoch in range(10):\n",
    "\n",
    "    train_loss = train(epoch, criterion, model, optimizer, train_loader)\n",
    "    val_loss = validate(criterion, model, val_loader)\n",
    "    losses[\"train\"].append(train_loss)\n",
    "    losses[\"val\"].append(val_loss)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    plt.plot(losses[\"train\"], label=\"training loss\")\n",
    "    plt.plot(losses[\"val\"], label=\"validation loss\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.pause(0.000001)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
