{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        emb = self.embedding(x)\n",
    "        out, h = self.gru(emb, h)\n",
    "        out = self.fc(out)\n",
    "        return out, h\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \" Initialize the hidden state of the RNN to zeros\"\n",
    "        hidden = nn.Parameter(torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = \n",
    "embedding_dim = \n",
    "hidden_dim = \n",
    "output_dim = \n",
    "n_layers = \n",
    "drop_prob = \n",
    "\n",
    "model = Model(input_dim, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, batch_size, sequence_length, num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            state = model.init_hidden(batch_size) # Start with a new state in each batch\n",
    "            state = state.to(device)\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, state = model(x, state)\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch%30 == 0:\n",
    "                print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
    "                losses.append(loss.item())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "losses = train(train_loader, model, batch_size, sequence_length, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss evolution during training\n",
    "plt.plot(losses)\n",
    "plt.title('model loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
