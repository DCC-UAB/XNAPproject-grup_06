{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "from itertools import groupby\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('/kaggle/input/handwriting-recognition/written_name_train_v2.csv')\n",
    "val_csv = pd.read_csv('/kaggle/input/handwriting-recognition/written_name_validation_v2.csv')\n",
    "train_img_dir = '/kaggle/input/handwriting-recognition/train_v2/train'\n",
    "val_img_dir = '/kaggle/input/handwriting-recognition/validation_v2/validation'\n",
    "print(train_csv.shape[0], val_csv.shape[0])\n",
    "train_csv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization_data(data_csv, image_dir):\n",
    "    # Set up the plot\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(15, 8))\n",
    "    \n",
    "    # Loop over the random images and plot the bounding boxes\n",
    "    for i in range(9):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        \n",
    "        idx = random.randint(0, 1000)\n",
    "        # Load the image\n",
    "        image_path = os.path.join(image_dir, data_csv.loc[idx, 'FILENAME'])\n",
    "        image = cv2.imread(image_path, 0)\n",
    "        axs[row, col].imshow(image, cmap = 'gray')\n",
    "        axs[row, col].set_title(data_csv.loc[idx, 'IDENTITY'], fontsize=10)\n",
    "        axs[row, col].axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "visualization_data(train_csv, train_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = train_csv[train_csv['IDENTITY'] != 'UNREADABLE']\n",
    "val_csv = val_csv[val_csv['IDENTITY'] != 'UNREADABLE']\n",
    "print(train_csv.shape[0], val_csv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = string.ascii_uppercase + \"' -\"\n",
    "max_str_len = 24 # max length of input labels\n",
    "num_of_characters = len(alphabets) + 1\n",
    "num_of_timestamps = 64 # max length of predicted labels: 64\n",
    "batch_size = 512\n",
    "\n",
    "def encode_to_label(text):\n",
    "    # Encoding each output word into digits\n",
    "    dig_list = []\n",
    "    for char in str(text):\n",
    "        idx = alphabets.find(char)\n",
    "        dig_list.append(idx if idx!=-1 else alphabets.find('-'))\n",
    "    \n",
    "    return pad_sequences([dig_list], maxlen=max_str_len, padding='post', value=-1)[0]\n",
    "\n",
    "name = 'QUY DAU'\n",
    "print(name, '\\n', encode_to_label(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_to_text(number_arr):\n",
    "    text = ''\n",
    "    for number in number_arr:\n",
    "        if number == -1: # CTC blank\n",
    "            break\n",
    "        else:\n",
    "            text += alphabets[number]\n",
    "\n",
    "    return text\n",
    "\n",
    "decode_to_text([16, 20, 24, 27, 3, 0, 20, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def preprocess_image(image_path, label, label_len):\n",
    "    # Read image\n",
    "    file = tf.io.read_file(image_path)\n",
    "    # Decode and convert to grayscale\n",
    "    image = tf.image.decode_png(file, channels=1)\n",
    "    # Convert to float32 in [0, 1] range\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # Resize the image\n",
    "    image = tf.image.resize(image, [64, 256])\n",
    "    # Compute the input_len\n",
    "    input_len = [num_of_timestamps - 2]\n",
    "    \n",
    "    return image, label, input_len, label_len, [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Function for Create list of image data\n",
    "def create_data_list(data_csv, img_dir):\n",
    "    data_x, data_y, label_len = [], [], []\n",
    "    for idx, row in data_csv.iterrows():\n",
    "        if isinstance(row['IDENTITY'], str):\n",
    "            data_x.append(os.path.join(img_dir, row['FILENAME']))\n",
    "            text = row['IDENTITY'].upper()\n",
    "            data_y.append(encode_to_label(text))\n",
    "            label_len.append(len(text))\n",
    "        \n",
    "    return data_x, data_y, label_len\n",
    "\n",
    "# Function for Create tensorflow dataset\n",
    "def create_tf_dataset(data_x, data_y, label_len):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data_x, data_y, label_len))\n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls=AUTOTUNE).batch(batch_size)\n",
    "    dataset = dataset.map(lambda *x: (x[0:-1],x[-1])).prefetch(AUTOTUNE).prefetch(buffer_size=AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensorflow Dataset for train data\n",
    "train_x, train_y, train_label_len = create_data_list(train_csv, train_img_dir)\n",
    "train_dataset = create_tf_dataset(train_x, train_y, train_label_len)\n",
    "\n",
    "# Create tensorflow Dataset for val data\n",
    "val_x, val_y, val_label_len = create_data_list(val_csv, val_img_dir)\n",
    "val_dataset = create_tf_dataset(val_x, val_y, val_label_len)\n",
    "\n",
    "print(len(train_y), len(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the CTC loss function\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    " \n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(num_filters, use_max_pool=True, pool_size=(4, 2), stride=2):\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Conv2D(num_filters, (3, 3), activation=\"relu\", padding=\"same\"))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(MaxPool2D(pool_size=pool_size, strides=stride))\n",
    "    return cnn\n",
    "    \n",
    "# Define the CRNN Model\n",
    "def CRNN():\n",
    "    # Inputs\n",
    "    inputs = Input(shape=(64, 256, 1), name='image')\n",
    "    labels = Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "    \n",
    "    # CNN\n",
    "    conv1 = conv_block(32)(inputs)\n",
    "    conv2 = conv_block(64)(conv1)\n",
    "    conv3 = conv_block(128, pool_size=(4, 1), stride=1)(conv2)\n",
    "    conv4 = conv_block(256, pool_size=(4, 1), stride=1)(conv3)\n",
    "    conv5 = conv_block(128, pool_size=(5, 1), stride=1)(conv4)\n",
    "\n",
    "    # CNN --> RNN\n",
    "    reshape = Reshape((num_of_timestamps, 512))(conv5)\n",
    "    dense = Dense(num_of_timestamps, activation='relu')(reshape)\n",
    "    bn = BatchNormalization()(dense)\n",
    "    dropout = Dropout(0.2)(bn)\n",
    "    \n",
    "    # RNN\n",
    "    lstm1 = Bidirectional(LSTM(64, return_sequences=True, dropout = 0.2))(dropout)\n",
    "    lstm2 = Bidirectional(LSTM(64, return_sequences=True, dropout = 0.2))(lstm1)\n",
    "    \n",
    "    # Outputs\n",
    "    y_pred = Dense(num_of_characters, activation = 'softmax')(lstm2)\n",
    "    ctc_loss = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "    \n",
    "    pred_model = Model(inputs=inputs, outputs=y_pred)\n",
    "    train_model = Model(inputs=[inputs, labels, input_length, label_length], outputs=ctc_loss)\n",
    "    \n",
    "    return pred_model, train_model\n",
    "\n",
    "model, train_model = CRNN()\n",
    "train_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "train_model.compile(loss={'ctc': lambda y_true, y_pred: y_pred},\n",
    "                    optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, clipnorm=1.0))\n",
    "\n",
    "filepath = \"/kaggle/working/best_model.h5\"\n",
    "\n",
    "# function callback\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor= 'val_loss',\n",
    "                             verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', mode='auto', patience=10)\n",
    "callbacks_list = [checkpoint, earlyStopping]\n",
    "\n",
    "# method 1\n",
    "history = train_model.fit(train_dataset,\n",
    "                          epochs=30,\n",
    "                          validation_data=val_dataset,\n",
    "                          verbose = 1,\n",
    "                          shuffle=True, \n",
    "                          callbacks=callbacks_list)\n",
    "\n",
    "# Save the model\n",
    "# model.save('/kaggle/working/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training process\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Training Loss vs Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "# model.load_weights('/kaggle/working/best_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_decoder(predictions):\n",
    "    '''\n",
    "    input: given batch of predictions from text rec model\n",
    "    output: return lists of raw extracted text\n",
    "\n",
    "    '''\n",
    "    text_list = []\n",
    "    \n",
    "    pred_indices = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    for i in range(pred_indices.shape[0]):\n",
    "        ans = \"\"\n",
    "        \n",
    "        ## merge repeats\n",
    "        merged_list = [k for k,_ in groupby(pred_indices[i])]\n",
    "        \n",
    "        ## remove blanks\n",
    "        for p in merged_list:\n",
    "            if p != len(alphabets):\n",
    "                ans += alphabets[int(p)]\n",
    "        \n",
    "        text_list.append(ans)\n",
    "        \n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_labels(dataset, model):\n",
    "    \"\"\"\n",
    "    Input: Tensorflow Dataset and the model to predict\n",
    "    Output: Extract labels from dataset and get prediction from the model\n",
    "    \"\"\"\n",
    "    predictions, labels = [], []\n",
    "    for batch in dataset.as_numpy_iterator():\n",
    "        images, num_labels = batch[0][0], batch[0][1]\n",
    "        preds = model.predict(images)\n",
    "        predictions.extend(ctc_decoder(preds))\n",
    "        labels.extend([decode_to_text(num_labels[i]) for i in range(len(num_labels))])\n",
    "    \n",
    "    return predictions, labels\n",
    "\n",
    "predictions, labels = get_predictions_and_labels(val_dataset, model)\n",
    "print(len(predictions), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_characters_and_words(predictions, labels):\n",
    "    \"\"\"\n",
    "    Input: The predictions and labels text of validation data\n",
    "    Output: The accuracy about words and characters\n",
    "    \"\"\"\n",
    "    word_accuracy = np.mean([predictions[i] == labels[i] for i in range(len(predictions))])\n",
    "    num_correct_char, len_char = 0, 0\n",
    "    for i in range(len(predictions)):\n",
    "        num_correct_char += np.sum([predictions[i][j] == labels[i][j] for j in range(min(len(predictions[i]), len(labels[i])))])\n",
    "        len_char += len(labels[i])\n",
    "    \n",
    "    character_accuracy = num_correct_char/ float(len_char)\n",
    "    return word_accuracy, character_accuracy\n",
    "\n",
    "word_accuracy, character_accuracy = get_accuracy_characters_and_words(predictions, labels)\n",
    "print('Character accuracy in the validation set: {} %'.format(round(character_accuracy * 100, 3)))\n",
    "print('Word accuracy in the validation set: {} %'.format(round(word_accuracy * 100, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare image directory and label csv file for the test dataset\n",
    "test_img_dir = '/kaggle/input/handwriting-recognition/test_v2/test'\n",
    "test_csv = pd.read_csv('/kaggle/input/handwriting-recognition/written_name_test_v2.csv')\n",
    "\n",
    "# Create test dataset with TensorFlow format\n",
    "test_x, test_y, test_label_len = create_data_list(test_csv, test_img_dir)\n",
    "test_dataset = create_tf_dataset(test_x, test_y, test_label_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_images_data_with_model_prediction(model, dataset):\n",
    "    # Create predictions for a batch in this dataset\n",
    "    for batch in dataset.as_numpy_iterator():\n",
    "        images = batch[0][0]\n",
    "        predictions = ctc_decoder(model.predict(images))\n",
    "        break\n",
    "    \n",
    "    # Set up the plot\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(15, 8))\n",
    "    \n",
    "    # Loop over the random images and plot the image with prediction model\n",
    "    for i in range(9):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        \n",
    "        idx = random.randint(0, batch_size)\n",
    "        axs[row, col].imshow(images[idx], cmap = 'gray')\n",
    "        axs[row, col].set_title('Prediction: {}'.format(predictions[idx]), fontsize=12)\n",
    "        axs[row, col].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "test_images_data_with_model_prediction(model, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
