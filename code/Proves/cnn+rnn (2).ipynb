{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load the data"
      ],
      "metadata": {
        "id": "Q3VBhPX4fOzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUiXmdNKfXu3",
        "outputId": "5735eefa-a484-4882-8e5f-dd415dc0d194"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d landlord/handwriting-recognition\n",
        "!unzip handwriting-recognition.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txawye17jf0Y",
        "outputId": "417ccea3-5126-48b5-eb12-ff689fef20e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/landlord/handwriting-recognition\n",
            "License(s): CC0-1.0\n",
            "Downloading handwriting-recognition.zip to /content\n",
            "100% 1.26G/1.26G [01:07<00:00, 19.6MB/s]\n",
            "100% 1.26G/1.26G [01:07<00:00, 20.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the libraries"
      ],
      "metadata": {
        "id": "ogcWlikvfvAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import cv2\n",
        "import imghdr\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from itertools import chain\n",
        "import random\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "dyxOFiHgfx2W"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean the data"
      ],
      "metadata": {
        "id": "fJo51fqCyOn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('written_name_train_v2.csv')\n",
        "validation = pd.read_csv('written_name_validation_v2.csv')"
      ],
      "metadata": {
        "id": "L5djmycPimhV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of NaNs in train set      : \", train['IDENTITY'].isnull().sum())\n",
        "print(\"Number of NaNs in validation set : \", validation['IDENTITY'].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfg9PssWySTh",
        "outputId": "21887058-a68d-4302-e3d9-b10ed5421c36"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaNs in train set      :  565\n",
            "Number of NaNs in validation set :  78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.dropna(axis=0, inplace=True)\n",
        "validation.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "KFCQgpFIygD4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train[train['IDENTITY'] != 'UNREADABLE']\n",
        "valid_data = validation[validation['IDENTITY'] != 'UNREADABLE']"
      ],
      "metadata": {
        "id": "G7wR_R1HymOm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['IDENTITY'] = train_data['IDENTITY'].str.upper()\n",
        "valid_data['IDENTITY'] = valid_data['IDENTITY'].str.upper()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aXOYk4ayop_",
        "outputId": "17058112-8959-4d73-97bb-eab9407b4346"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-8fe6abeeb9f2>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['IDENTITY'] = train_data['IDENTITY'].str.upper()\n",
            "<ipython-input-11-8fe6abeeb9f2>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  valid_data['IDENTITY'] = valid_data['IDENTITY'].str.upper()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.reset_index(inplace = True, drop=True)\n",
        "valid_data.reset_index(inplace = True, drop=True)"
      ],
      "metadata": {
        "id": "0kkBz9Niy5TR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initiate Data Loader"
      ],
      "metadata": {
        "id": "tJATATG-gvSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(Dataset):\n",
        "    def __init__(self, dataframe, path, char_map, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.path = path\n",
        "        self.char_map = char_map\n",
        "        # self.img_size = img_size\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe['FILENAME'].values[idx]\n",
        "        img = cv2.imread(os.path.join(self.path, img_path), cv2.IMREAD_GRAYSCALE)\n",
        "        # img = cv2.resize(img, self.img_size)\n",
        "        img = img / 255.0\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        img = torch.tensor(img, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        text = str(self.dataframe['IDENTITY'].values[idx])\n",
        "        label = torch.tensor([self.char_map[char] for char in text], dtype=torch.long)\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "yHNq8sa0hJ1M"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(set(\"\".join(train['IDENTITY'].astype(str)).upper()))\n",
        "char_to_label = {char: idx for idx, char in enumerate(characters)}\n",
        "label_to_char = {idx: char for char, idx in char_to_label.items()}\n",
        "\n",
        "print(char_to_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwZLRgRTlrJm",
        "outputId": "5c9f5a5e-fffa-4dec-e5a2-638bff926f70"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, \"'\": 1, '-': 2, 'A': 3, 'B': 4, 'C': 5, 'D': 6, 'E': 7, 'F': 8, 'G': 9, 'H': 10, 'I': 11, 'J': 12, 'K': 13, 'L': 14, 'M': 15, 'N': 16, 'O': 17, 'P': 18, 'Q': 19, 'R': 20, 'S': 21, 'T': 22, 'U': 23, 'V': 24, 'W': 25, 'X': 26, 'Y': 27, 'Z': 28, '`': 29}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "pl0Neo4ChPUM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_train = \"train_v2/train/\"\n",
        "path_valid = \"validation_v2/validation/\""
      ],
      "metadata": {
        "id": "mRUNj75skdVe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DataGenerator(train_data,path_train,char_to_label)\n",
        "valid_dataset = DataGenerator(valid_data,path_valid,char_to_label)"
      ],
      "metadata": {
        "id": "sSPFaVRniZw2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "MkfDcOh3g0iP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (images, labels) in enumerate(train_loader):\n",
        "    print(f\"Batch {i}:\")\n",
        "    print(images.shape)  # This will give you the shape of the batch of images\n",
        "    print(labels.shape)  # And this will give you the shape of the batch of labels\n",
        "    break  # Just to demonstrate, stop after the first batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbZNelU6pBZh",
        "outputId": "ef721a81-01df-4d58-e777-64442d6a77fb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0:\n",
            "torch.Size([1, 1, 31, 284])\n",
            "torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segment the images"
      ],
      "metadata": {
        "id": "DICRIbWhnsr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_words(image_tensor, label):\n",
        "    \"\"\"\n",
        "    Given an image tensor and a label tensor, this function extracts letters from the image\n",
        "    and returns a list of segmented letter images.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    image_tensor : PyTorch tensor of the image to classify\n",
        "    label : PyTorch tensor representing the word as a sequence of character numbers\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of numpy arrays: Each array represents a segmented letter image.\n",
        "    \"\"\"\n",
        "\n",
        "    segmented_letters = []\n",
        "\n",
        "    # Convert the image tensor to a NumPy array\n",
        "    image = image_tensor.squeeze().numpy()\n",
        "    image = (image * 255).astype('uint8')  # Assuming the tensor is normalized [0, 1]\n",
        "\n",
        "    # Convert the image to grayscale if it's not already\n",
        "    if image.ndim == 3:\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "        gray = image\n",
        "\n",
        "    # Apply threshold to binarize the image\n",
        "    _, thresh = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Find contours\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Sort contours from left to right\n",
        "    contours = sorted(contours, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
        "\n",
        "    # Convert the label tensor to a list of integers\n",
        "    label_list = label.flatten().tolist()\n",
        "\n",
        "\n",
        "    # Check if the number of contours matches the length of the label\n",
        "    if len(contours) == len(label_list):\n",
        "        # Iterate over the contours and extract each letter as a separate image\n",
        "        for i, contour in enumerate(contours):\n",
        "            # Get the bounding box of the contour\n",
        "            x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "            # Extract the letter\n",
        "            letter_img = image[y:y+h, x:x+w]\n",
        "            # letter_tensor = torch.tensor(letter_img, dtype=torch.float32)\n",
        "            segmented_letters.append(letter_img)\n",
        "\n",
        "    return segmented_letters"
      ],
      "metadata": {
        "id": "wxhCDGg6KmQJ"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_and_filter(segmented_letters, target_size=(10, 14)):\n",
        "    \"\"\"\n",
        "    Resize segmented letter images to the target size while maintaining aspect ratio.\n",
        "    Parameters\n",
        "    ----------\n",
        "    segmented_letters : list of PyTorch tensors\n",
        "        Each tensor represents a segmented letter image.\n",
        "    target_size : tuple, optional\n",
        "        Target size for resizing the segmented letter images. Default is (10, 14).\n",
        "    Returns\n",
        "    -------\n",
        "    list of PyTorch tensors: Each tensor represents a resized segmented letter image.\n",
        "    \"\"\"\n",
        "    resized_letters = []\n",
        "\n",
        "    for letter_img in segmented_letters:\n",
        "        # Get the dimensions of the image\n",
        "        height, width = letter_img.shape\n",
        "\n",
        "        # Calculate the aspect ratio\n",
        "        aspect_ratio = width / height\n",
        "\n",
        "        # Determine new dimensions maintaining the aspect ratio\n",
        "        if aspect_ratio > 1:  # Width is greater than height\n",
        "            new_width = min(target_size[1], int(target_size[0] * aspect_ratio))\n",
        "            new_height = min(target_size[0], int(new_width / aspect_ratio))\n",
        "        else:  # Height is greater than or equal to width\n",
        "            new_height = min(target_size[0], int(target_size[1] / aspect_ratio))\n",
        "            new_width = min(target_size[1], int(new_height * aspect_ratio))\n",
        "\n",
        "        # Resize the image to new dimensions\n",
        "        resized_img = cv2.resize(letter_img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # Create a blank white image of target size\n",
        "        new_img = np.ones((target_size[0], target_size[1]), dtype=np.uint8) * 255\n",
        "\n",
        "        # Calculate the offset to paste the resized image\n",
        "        x_offset = (target_size[1] - new_width) // 2\n",
        "        y_offset = (target_size[0] - new_height) // 2\n",
        "\n",
        "        # Paste the resized image onto the blank white image\n",
        "        new_img[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_img\n",
        "\n",
        "        # Convert the resized image back to a PyTorch tensor\n",
        "        resized_tensor = torch.tensor(new_img, dtype=torch.float32)\n",
        "\n",
        "        resized_letters.append(resized_tensor)\n",
        "\n",
        "    return resized_letters"
      ],
      "metadata": {
        "id": "8jXm7GCoNI7I"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_loader:\n",
        "    letters = segment_words(images, labels)\n",
        "    if len(letters) == 0:\n",
        "        continue\n",
        "    else:\n",
        "        break\n",
        "letters = resize_and_filter(letters)\n",
        "print(letters!=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbAXT49hG2HW",
        "outputId": "71cc9117-4122-4f35-81bc-75ff8f15bc45"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_letters = len(letters)\n",
        "\n",
        "if num_letters == 0:\n",
        "    print(\"No letters to display.\")\n",
        "\n",
        "\n",
        "for i, letter_img in enumerate(letters):\n",
        "\n",
        "    plt.figure(figsize=(2, 2))\n",
        "    plt.imshow(letter_img, cmap='gray')\n",
        "    plt.title(f'Letter {i+1}')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "q5oFIhvCGvMW",
        "outputId": "267be346-3d48-4051-e389-d2c88888eb1e"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the pre-trained CNN model"
      ],
      "metadata": {
        "id": "qE6gzOImfqgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define SimpleCNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \"\"\"\n",
        "        Entren imatges amb 1 canal de color (grayscale) i amb aquesta capa convolucional\n",
        "        s'estreuen característiques loclas de la imatge detectant patrons com bordes,\n",
        "        textures i altres detalls que poden ser importants.\n",
        "        \"\"\"\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        \"\"\"\n",
        "        El max pooling redueix la dimensionalitat dels mapas de característiques,\n",
        "        disminuint la mida de les imatges i retenint les característiques més importants\n",
        "        Això ajuda a reduir el sobreajustament i millora l'eficàcia computacional.\n",
        "        \"\"\"\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \"\"\"\n",
        "        Les següents dues capes s'ecarreguen de la classificació final. Transformen\n",
        "        les característiques extretes per les capas convolucionals i de pooling a les\n",
        "        propietats de les diferents classes (lletres).\n",
        "        \"\"\"\n",
        "        self.fc1 = nn.Linear(16 * 7 * 5, 64)\n",
        "        self.fc2 = nn.Linear(64, 29)  # 29 perquè hi ha 29 caràcters a l'alfabet\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = x.view(-1, 16 * 7 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WU_IQ5YMec8S"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your trained CNN model\n",
        "cnn_model = SimpleCNN()\n",
        "cnn_model.load_state_dict(torch.load('simple_cnn_model.pth'))\n",
        "cnn_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i28RCsXHf3pk",
        "outputId": "4b1db46f-7fe5-4f6f-d89c-6a9e192748ca"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleCNN(\n",
              "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc1): Linear(in_features=560, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=29, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create SimpleLSTM model"
      ],
      "metadata": {
        "id": "T27Piy9hnn2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define SimpleLSTM model\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
        "        c_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h_0, c_0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "sS1_7PUFcWdl"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM model parameters\n",
        "input_size = 29  # This should match the CNN output size\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "output_size = 29  # Number of classes\n",
        "lstm_model = SimpleLSTM(input_size, hidden_size, num_layers, output_size)"
      ],
      "metadata": {
        "id": "5BzZvQmqWjnu"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "Y_l2eVj8nwQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizer for LSTM\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "0lpnCdUzmxKg"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "batch = 1\n",
        "num_epochs = 20\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []"
      ],
      "metadata": {
        "id": "_5DQiupQcaDJ"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    lstm_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Segment images\n",
        "        segmented_images = resize_and_filter(segment_words(images, labels))\n",
        "\n",
        "        if len(segmented_images) == 0:\n",
        "            continue\n",
        "\n",
        "        segmented_images_tensor = torch.stack(segmented_images)\n",
        "\n",
        "        # Print the shape of the tensor\n",
        "        print(\"Before reshaping:\", segmented_images_tensor.shape)\n",
        "\n",
        "        # Reshape the tensor with a batch size of 1\n",
        "        segmented_images_tensor = segmented_images_tensor.view(1, segmented_images_tensor.size(0), segmented_images_tensor.size(1), segmented_images_tensor.size(2))\n",
        "\n",
        "        # Print the shape of the tensor after reshaping\n",
        "        print(\"After reshaping:\", segmented_images_tensor.shape)\n",
        "\n",
        "        # CNN feature extraction\n",
        "        with torch.no_grad():\n",
        "            features = cnn_model(segmented_images_tensor)\n",
        "\n",
        "        # Reshape features for LSTM\n",
        "        features = features.view(batch, -1, features.size(1))\n",
        "\n",
        "        # RNN\n",
        "        outputs = lstm_model(features)\n",
        "\n",
        "        loss = criterion(outputs, labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.view(-1)).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = correct / total\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_accuracy)\n",
        "\n",
        "    # Validation\n",
        "    lstm_model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            segmented_images = resize_and_filter(segment_words(images, labels))\n",
        "            if len(segmented_images) == 0:\n",
        "                continue\n",
        "\n",
        "            segmented_images_tensor = torch.stack(segmented_images)\n",
        "            segmented_images = segmented_images.view(-1, segmented_images.size(2), segmented_images.size(3), segmented_images.size(4))\n",
        "            with torch.no_grad():\n",
        "                features = cnn_model(segmented_images)\n",
        "            features = features.view(batch, -1, features.size(1))\n",
        "            outputs = lstm_model(features)\n",
        "            loss = criterion(outputs, labels.view(-1))\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.view(-1)).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(valid_loader)\n",
        "    epoch_accuracy = correct / total\n",
        "    test_losses.append(epoch_loss)\n",
        "    test_accuracies.append(epoch_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, Test Acc: {test_accuracies[-1]:.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(lstm_model.state_dict(), 'cnn_rnn_model.pth')\n",
        "\n",
        "# Create loss and accuracy plots\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs+1), test_losses, label='Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs+1), train_accuracies, label='Training Accuracy')\n",
        "plt.plot(range(1, num_epochs+1), test_accuracies, label='Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "2S-BBQTccnSK",
        "outputId": "a953ad28-3418-42a6-eabc-1d16de919c16"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before reshaping: torch.Size([7, 10, 14])\n",
            "After reshaping: torch.Size([1, 7, 10, 14])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [16, 1, 3, 3], expected input[1, 7, 10, 14] to have 1 channels, but got 7 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-266-201e1f080eb3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# CNN feature extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmented_images_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Reshape features for LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-180-0f2600cc5b4c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 1, 3, 3], expected input[1, 7, 10, 14] to have 1 channels, but got 7 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    lstm_model.train()\n",
        "    for images, labels in train_loader:\n",
        "        # Segment the images\n",
        "        segmented_images = resize_and_filter(segment_words(images, labels))\n",
        "\n",
        "        if len(segmented_images) == 0:\n",
        "            continue\n",
        "\n",
        "        segmented_images_tensor = torch.stack(segmented_images)\n",
        "\n",
        "        # Extract features using CNN\n",
        "        with torch.no_grad():\n",
        "            features = cnn_model(segmented_images_tensor)  # (Batch*Segments, Embedding)\n",
        "            print(f\"Features shape: {features.shape}\")\n",
        "            features = features.view(1, -1, features.shape[-1])  # (1, Segments*Batch, Embedding)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through LSTM\n",
        "        outputs = lstm_model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Validation loop\n",
        "lstm_model.eval()\n",
        "incorrect_images = []\n",
        "incorrect_labels = []\n",
        "correct_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in valid_loader:\n",
        "        # Segment the images\n",
        "\n",
        "        segmented_images = segment_words(images, labels)\n",
        "\n",
        "        if len(segmented_images) == 0:\n",
        "            continue\n",
        "\n",
        "        # Extract features using CNN\n",
        "        features = cnn_model(segmented_images)  # (Batch*Segments, Embedding)\n",
        "        features = features.view(1, -1, 64)  # (1, Segments*Batch, Embedding)\n",
        "\n",
        "        # Forward pass through LSTM\n",
        "        outputs = lstm_model(features)\n",
        "\n",
        "        # Get predictions\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Collect incorrect predictions\n",
        "        if predicted != labels:\n",
        "            incorrect_images.append(images)\n",
        "            incorrect_labels.append(predicted.item())\n",
        "            correct_labels.append(labels.item())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Ks0LlF9HnDsT",
        "outputId": "303fe01c-2800-4316-c458-1f5b8de24dc8"
      },
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [16, 1, 3, 3], expected input[1, 8, 10, 14] to have 1 channels, but got 8 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-235-3cc12bec9abe>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Extract features using CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmented_images_tensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (Batch*Segments, Embedding)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Features shape: {features.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, Segments*Batch, Embedding)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-180-0f2600cc5b4c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 1, 3, 3], expected input[1, 8, 10, 14] to have 1 channels, but got 8 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KsS-lR7ya7fi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}