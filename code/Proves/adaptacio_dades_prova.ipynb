{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import imghdr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING AND VISUALIZATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/handwriting-recognition/written_name_train_v2.csv')\n",
    "test = pd.read_csv('/handwriting-recognition/written_name_validation_v2.csv')\n",
    "train_img_dir = '/handwriting-recognition/train_v2/train'\n",
    "test_img_dir = '/handwriting-recognition/validation_v2/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tain set: \",train['IDENTITY'].shape[0])\n",
    "print(\"UNREADABLE in train set : \", train['IDENTITY'].isnull().sum())\n",
    "print(\"Test set: \", test['IDENTITY'].shape[0])\n",
    "print(\"UNREADABLE in validation set : \", test['IDENTITY'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['IDENTITY'] != 'UNREADABLE']\n",
    "train = train.dropna()\n",
    "test = test[test['IDENTITY'] != 'UNREADABLE']\n",
    "test = test.dropna()\n",
    "print(\"train:\",train.shape[0],\" test:\",test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 3000\n",
    "valid_size = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data_img = []\n",
    "train_data_idt = []\n",
    "#for i in range(test.shape[0]):   #it's take too longe to do all images\n",
    "for i in range(300):\n",
    "    image_filename = train['FILENAME'].iloc[i] \n",
    "    image_path = os.path.join(train_img_dir,image_filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    image = to_binary(image)\n",
    "    train_data_img.append(image) \n",
    "    train_data_idt.append(train['IDENTITY'].iloc[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_img = []\n",
    "test_data_idt = []\n",
    "#for i in range(test.shape[0]):\n",
    "for i in range(1000):\n",
    "    image_filename = test['FILENAME'].iloc[i]\n",
    "    image_path = os.path.join(test_img_dir,image_filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    image = to_binary(image)\n",
    "    test_data_img.append(image)\n",
    "    test_data_idt.append(test['IDENTITY'].iloc[i])  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(train_data_idt[i])\n",
    "    print(train_data_img[i].shape)\n",
    "    plt.imshow(train_data_img[i],cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "for i in range(3):\n",
    "    print(test_data_idt[i])\n",
    "    print(test_data_img[i].shape)\n",
    "    plt.imshow(test_data_img[i],cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coding IDENTITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = train_data_idt + test_data_idt\n",
    "# Concatenate names into a single string\n",
    "all_characters = ''.join(all_data)\n",
    "\n",
    "# Create a set to get unique characters\n",
    "unique_characters_set = set(all_characters)\n",
    "\n",
    "# Convert the set back to a list if needed\n",
    "unique_characters_list = list(unique_characters_set)\n",
    "\n",
    "print(unique_characters_list)\n",
    "print(len(unique_characters_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "lb.fit(unique_characters_list)\n",
    "lb.classes_\n",
    "dec = {}\n",
    "for i in range(len(unique_characters_list)):\n",
    "    if unique_characters_list[i] in dec.keys():\n",
    "        pass\n",
    "    else: \n",
    "        a = lb.transform([unique_characters_list[i]])\n",
    "        a = list(chain(*a)) #flattened_list from [[....]] to []\n",
    "        dec[unique_characters_list[i]]= a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train data longest word :\")\n",
    "length_of_longest_word_in_train = len(max(train_data_idt, key=len))\n",
    "print(length_of_longest_word_in_train)\n",
    "word_with_longest_characters = max(train_data_idt, key=len)\n",
    "print(word_with_longest_characters)\n",
    "print()\n",
    "print(\"test data longest word :\")\n",
    "length_of_longest_word_in_test = len(max(test_data_idt, key=len))\n",
    "print(length_of_longest_word_in_test)\n",
    "word_with_longest_characters = max(test_data_idt, key=len)\n",
    "print(word_with_longest_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a name to a list of vectors using the dictionary\n",
    "def name_to_vectors(name,max_len,num_char):\n",
    "    zeros_list = [0] * num_char\n",
    "    num_vec_to_add = max_len - len(name)\n",
    "    list_vc_name = [dec[char] for char in name]\n",
    "    \n",
    "    for i in range(num_vec_to_add):\n",
    "        list_vc_name.append(zeros_list)\n",
    "    return list_vc_name\n",
    "\n",
    "# Creating a list of vectors for each name\n",
    "vectors_list_train_idt = [name_to_vectors(name,length_of_longest_word_in_train,len(unique_characters_list)) for name in train_data_idt] # for train data\n",
    "vectors_list_test_idt = [name_to_vectors(name,length_of_longest_word_in_train,len(unique_characters_list)) for name in test_data_idt] # for test data \n",
    "\n",
    "#for name, vectors in zip(names, vectors_list):\n",
    "    #print(f\"{name}: {vectors}\")\n",
    "vectors_list_train_idt = np.array(vectors_list_train_idt) #change from list to numpy array\n",
    "vectors_list_test_idt = np.array(vectors_list_test_idt)\n",
    "print(vectors_list_train_idt.shape)\n",
    "print(vectors_list_test_idt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LB = LabelBinarizer()\n",
    "#train_data_idt = LB.fit_transform(train_data_idt)\n",
    "#test_data_idt = LB.fit_transform(test_data_idt)\n",
    "#print(test_data_idt[0:5])\n",
    "\n",
    "train_data_img = np.array(train_data_img)\n",
    "train_data_img = train_data_img.reshape(-1,62,284,1)\n",
    "\n",
    "test_data_img = np.array(test_data_img)\n",
    "test_data_img = test_data_img.reshape(-1,62,284,1)\n",
    "\n",
    "#train_data_idt = np.array(train_data_idt)\n",
    "#test_data_idt = np.array(test_data_idt)\n",
    "\n",
    "#print(type(train_data_idt[1]), type(test_data_idt))\n",
    "print(train_data_img.shape,vectors_list_train_idt.shape)\n",
    "print(test_data_img.shape,vectors_list_test_idt.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
